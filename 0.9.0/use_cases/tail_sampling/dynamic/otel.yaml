apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  labels:
    mydecisive.ai/hub-name: mdaihub-ts
  name: trace-balancer
  namespace: mdai
spec:
  serviceAccount: loadbalancer
  image: otel/opentelemetry-collector-contrib:latest
  replicas: 1
  resources:
    limits:
      memory: "256Mi"
      cpu: "200m"
    requests:
      memory: "128Mi"
      cpu: "100m"
  config:
    receivers:
      otlp/from_agent:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"

    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"

    processors:
      memory_limiter:
        # Keep this safely below the pod limit (256Mi). Tune as needed.
        limit_mib: 200
        spike_limit_mib: 50
        check_interval: 1s

      batch:
        send_batch_size: 200
        send_batch_max_size: 1000
        timeout: 5s

      deltatocumulative: {}

    exporters:
      loadbalancing:
        routing_key: traceID
        protocol:
          otlp:
            compression: gzip
            tls:
              insecure: true
        resolver:
          k8s:
            service: gateway-collector-headless.mdai

      prometheus:
        endpoint: "0.0.0.0:8899"
        metric_expiration: 180m
        resource_to_telemetry_conversion:
          enabled: true

    connectors:
      count/service:
        spans:
          trace.span.service.total:
            description: Total span count by service (all spans).
          trace.span.service.root:
            description: Root span count by service.
            conditions:
              - 'IsRootSpan()'

    service:
      telemetry:
        resource:
          mdai-logstream: collector
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8888
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp/from_agent]
          processors: [memory_limiter, batch]
          exporters: [loadbalancing, count/service]
        metrics:
          receivers: [count/service]
          processors: [deltatocumulative]
          exporters: [prometheus]

---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: gateway
  namespace: mdai
  labels:
    mydecisive.ai/hub-name: mdaihub-ts
spec:
  managementState: managed
  image: otel/opentelemetry-collector-contrib:latest
  replicas: 1
  resources:
    limits:
      memory: "512Mi"
      cpu: "200m"
    requests:
      memory: "256Mi"
      cpu: "100m"
  envFrom:
    - configMapRef:
        name: mdaihub-ts-variables
  config:
    receivers:
      otlp/from_lb:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317

    extensions:
      health_check:
        endpoint: "${env:MY_POD_IP}:13133"

    processors:
      memory_limiter:
        limit_mib: 420
        spike_limit_mib: 80
        check_interval: 1s

      tail_sampling:
        decision_wait: 10s
        policies:
          - name: keep-all-the-errors
            type: and
            and:
              and_sub_policy:
                - name: error-sampling
                  type: status_code
                  status_code:
                    status_codes: [ERROR]
          - name: probabilistic-sample-top-talkers
            type: and
            and:
              and_sub_policy:
                - name: services-using-tail-sampling
                  type: string_attribute
                  string_attribute:
                    key: operation
                    values: ["high-volume-op"]
                - name: probabilistic-policy
                  type: probabilistic
                  probabilistic:
                    sampling_percentage: 10
          - name: always-sample-non-top-talkers
            type: ottl_condition
            ottl_condition:
              error_mode: ignore
              span:
                - "attributes[\"operation\"] != \"high-volume-op\""

      batch:
        send_batch_size: 50
        send_batch_max_size: 200
        timeout: 10s

    exporters:
      debug/to_vendor:
        verbosity: detailed

    service:
      telemetry:
        resource:
          mdai-logstream: collector
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8888
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp/from_lb]
          processors: [memory_limiter, tail_sampling, batch]
          exporters: [debug/to_vendor]
